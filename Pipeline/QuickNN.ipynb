{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:kwzi4hkg) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34523f29e52f4ebebdc0ce4b6981c473",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Training Loss</td><td>▁</td></tr><tr><td>Validation Loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Training Loss</td><td>55546.69125</td></tr><tr><td>Validation Loss</td><td>3292.66591</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">clean-brook-5</strong> at: <a href='https://wandb.ai/turlagheoin/Count_cells/runs/kwzi4hkg' target=\"_blank\">https://wandb.ai/turlagheoin/Count_cells/runs/kwzi4hkg</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231206_022935-kwzi4hkg\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:kwzi4hkg). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca293ba4a74049dc89923eeba8360f84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011288888888925108, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\turla\\Documents\\4. Uliege\\Semester 3\\Computer Vision\\Cell-detection-and-counting-Algorithms\\Pipeline\\wandb\\run-20231206_023413-2zvpx369</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/turlagheoin/Count_cells/runs/2zvpx369' target=\"_blank\">electric-eon-6</a></strong> to <a href='https://wandb.ai/turlagheoin/Count_cells' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/turlagheoin/Count_cells' target=\"_blank\">https://wandb.ai/turlagheoin/Count_cells</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/turlagheoin/Count_cells/runs/2zvpx369' target=\"_blank\">https://wandb.ai/turlagheoin/Count_cells/runs/2zvpx369</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150, Training Loss: 56644.2733, Validation Loss: 2536.9677\n",
      "Epoch 2/150, Training Loss: 55171.0372, Validation Loss: 415.7535\n",
      "Epoch 3/150, Training Loss: 54429.0211, Validation Loss: 408.9879\n",
      "Epoch 4/150, Training Loss: 53843.8567, Validation Loss: 450.4972\n",
      "Epoch 5/150, Training Loss: 52149.8096, Validation Loss: 2453.1691\n",
      "Epoch 6/150, Training Loss: 50835.2037, Validation Loss: 599.6057\n",
      "Epoch 7/150, Training Loss: 52760.2496, Validation Loss: 3156.8141\n",
      "Epoch 8/150, Training Loss: 52045.5743, Validation Loss: 649.4500\n",
      "Epoch 9/150, Training Loss: 51739.8051, Validation Loss: 531.7023\n",
      "Epoch 10/150, Training Loss: 51520.6210, Validation Loss: 4608.3844\n",
      "Epoch 11/150, Training Loss: 51556.1190, Validation Loss: 424.8141\n",
      "Epoch 12/150, Training Loss: 51585.3427, Validation Loss: 292.5317\n",
      "Epoch 13/150, Training Loss: 51417.7870, Validation Loss: 281.2149\n",
      "Epoch 14/150, Training Loss: 51710.4152, Validation Loss: 286.1792\n",
      "Epoch 15/150, Training Loss: 51608.1257, Validation Loss: 578.4853\n",
      "Epoch 16/150, Training Loss: 51688.8785, Validation Loss: 729.4567\n",
      "Epoch 17/150, Training Loss: 51543.4662, Validation Loss: 270.2798\n",
      "Epoch 18/150, Training Loss: 52125.6476, Validation Loss: 2277.8187\n",
      "Epoch 19/150, Training Loss: 51780.7869, Validation Loss: 451.8658\n",
      "Epoch 20/150, Training Loss: 51899.3356, Validation Loss: 463.5816\n",
      "Epoch 21/150, Training Loss: 51251.7343, Validation Loss: 1719.9762\n",
      "Epoch 22/150, Training Loss: 51322.3144, Validation Loss: 269.2765\n",
      "Epoch 23/150, Training Loss: 51280.9322, Validation Loss: 268.7132\n",
      "Epoch 24/150, Training Loss: 51415.2993, Validation Loss: 279.2380\n",
      "Epoch 25/150, Training Loss: 51384.8203, Validation Loss: 586.3870\n",
      "Epoch 26/150, Training Loss: 51357.8595, Validation Loss: 322.9609\n",
      "Epoch 27/150, Training Loss: 51414.7872, Validation Loss: 345.2929\n",
      "Epoch 28/150, Training Loss: 51250.3809, Validation Loss: 358.7267\n",
      "Epoch 29/150, Training Loss: 51361.1200, Validation Loss: 280.1484\n",
      "Epoch 30/150, Training Loss: 51294.8775, Validation Loss: 1840.1907\n",
      "Epoch 31/150, Training Loss: 51526.0925, Validation Loss: 261.7362\n",
      "Epoch 32/150, Training Loss: 51326.0539, Validation Loss: 996.1895\n",
      "Epoch 33/150, Training Loss: 51204.7664, Validation Loss: 398.3300\n",
      "Epoch 34/150, Training Loss: 51192.8612, Validation Loss: 1020.4566\n",
      "Epoch 35/150, Training Loss: 51030.4530, Validation Loss: 285.1637\n",
      "Epoch 36/150, Training Loss: 51348.6858, Validation Loss: 275.1981\n",
      "Epoch 37/150, Training Loss: 51034.0047, Validation Loss: 297.2433\n",
      "Epoch 38/150, Training Loss: 51169.8611, Validation Loss: 268.0803\n",
      "Epoch 39/150, Training Loss: 51015.8078, Validation Loss: 1388.4438\n",
      "Epoch 40/150, Training Loss: 51196.0971, Validation Loss: 1062.2174\n",
      "Epoch 41/150, Training Loss: 51077.5647, Validation Loss: 1287.7888\n",
      "Epoch 42/150, Training Loss: 51090.6256, Validation Loss: 314.8601\n",
      "Epoch 43/150, Training Loss: 51178.3459, Validation Loss: 348.6883\n",
      "Epoch 44/150, Training Loss: 51189.0284, Validation Loss: 993.5011\n",
      "Epoch 45/150, Training Loss: 51254.8758, Validation Loss: 283.0486\n",
      "Epoch 46/150, Training Loss: 51149.5104, Validation Loss: 282.2308\n",
      "Epoch 47/150, Training Loss: 51025.0056, Validation Loss: 786.6799\n",
      "Epoch 48/150, Training Loss: 50997.1767, Validation Loss: 248.8636\n",
      "Epoch 49/150, Training Loss: 50752.9992, Validation Loss: 529.1794\n",
      "Epoch 50/150, Training Loss: 51100.7311, Validation Loss: 670.3456\n",
      "Epoch 51/150, Training Loss: 51238.6381, Validation Loss: 262.9778\n",
      "Epoch 52/150, Training Loss: 51045.8047, Validation Loss: 262.8928\n",
      "Epoch 53/150, Training Loss: 51201.6671, Validation Loss: 269.1927\n",
      "Epoch 54/150, Training Loss: 48470.9182, Validation Loss: 10302.7096\n",
      "Epoch 55/150, Training Loss: 51668.1586, Validation Loss: 565.5487\n",
      "Epoch 56/150, Training Loss: 51206.2326, Validation Loss: 693.6093\n",
      "Epoch 57/150, Training Loss: 51116.7257, Validation Loss: 1106.0906\n",
      "Epoch 58/150, Training Loss: 51095.4364, Validation Loss: 800.6164\n",
      "Epoch 59/150, Training Loss: 50925.2760, Validation Loss: 992.3137\n",
      "Epoch 60/150, Training Loss: 49766.0468, Validation Loss: 7419.4121\n",
      "Epoch 61/150, Training Loss: 51327.3149, Validation Loss: 537.2470\n",
      "Epoch 62/150, Training Loss: 51007.3293, Validation Loss: 1366.0998\n",
      "Epoch 63/150, Training Loss: 51150.2654, Validation Loss: 313.5261\n",
      "Epoch 64/150, Training Loss: 51191.5885, Validation Loss: 796.5411\n",
      "Epoch 65/150, Training Loss: 50812.5849, Validation Loss: 276.5109\n",
      "Epoch 66/150, Training Loss: 51108.1471, Validation Loss: 296.3577\n",
      "Epoch 67/150, Training Loss: 51110.1827, Validation Loss: 313.3470\n",
      "Epoch 68/150, Training Loss: 51084.5907, Validation Loss: 283.7308\n",
      "Epoch 69/150, Training Loss: 51071.7044, Validation Loss: 242.8945\n",
      "Epoch 70/150, Training Loss: 50881.7004, Validation Loss: 1782.7038\n",
      "Epoch 71/150, Training Loss: 51055.6344, Validation Loss: 475.9327\n",
      "Epoch 72/150, Training Loss: 51108.8835, Validation Loss: 1615.8777\n",
      "Epoch 73/150, Training Loss: 51141.3399, Validation Loss: 1551.4041\n",
      "Epoch 74/150, Training Loss: 51207.9076, Validation Loss: 617.2558\n",
      "Epoch 75/150, Training Loss: 51102.4704, Validation Loss: 1162.0102\n",
      "Epoch 76/150, Training Loss: 51127.1170, Validation Loss: 292.7240\n",
      "Epoch 77/150, Training Loss: 49863.8663, Validation Loss: 240.1935\n",
      "Epoch 78/150, Training Loss: 51323.2514, Validation Loss: 236.9289\n",
      "Epoch 79/150, Training Loss: 50685.8063, Validation Loss: 263.0787\n",
      "Epoch 80/150, Training Loss: 50211.1132, Validation Loss: 373.0991\n",
      "Epoch 81/150, Training Loss: 51073.7750, Validation Loss: 281.6621\n",
      "Epoch 82/150, Training Loss: 51000.2023, Validation Loss: 247.8596\n",
      "Epoch 83/150, Training Loss: 51010.1359, Validation Loss: 285.1096\n",
      "Epoch 84/150, Training Loss: 51002.1921, Validation Loss: 254.3125\n",
      "Epoch 85/150, Training Loss: 51080.8408, Validation Loss: 270.7832\n",
      "Epoch 86/150, Training Loss: 50750.0770, Validation Loss: 252.2057\n",
      "Epoch 87/150, Training Loss: 51118.0142, Validation Loss: 431.3650\n",
      "Epoch 88/150, Training Loss: 50970.0076, Validation Loss: 409.1600\n",
      "Epoch 89/150, Training Loss: 51180.7868, Validation Loss: 584.4864\n",
      "Epoch 90/150, Training Loss: 51078.4706, Validation Loss: 307.5980\n",
      "Epoch 91/150, Training Loss: 51112.6665, Validation Loss: 351.8107\n",
      "Epoch 92/150, Training Loss: 49535.5848, Validation Loss: 331.0137\n",
      "Epoch 93/150, Training Loss: 51321.2295, Validation Loss: 311.6932\n",
      "Epoch 94/150, Training Loss: 51051.8396, Validation Loss: 1696.1040\n",
      "Epoch 95/150, Training Loss: 50960.0000, Validation Loss: 958.0774\n",
      "Epoch 96/150, Training Loss: 50184.0505, Validation Loss: 317.5613\n",
      "Epoch 97/150, Training Loss: 51032.5445, Validation Loss: 899.1072\n",
      "Epoch 98/150, Training Loss: 51070.0977, Validation Loss: 256.8323\n",
      "Epoch 99/150, Training Loss: 51172.9068, Validation Loss: 976.5477\n",
      "Epoch 100/150, Training Loss: 51012.1801, Validation Loss: 253.1835\n",
      "Epoch 101/150, Training Loss: 50720.1781, Validation Loss: 801.2219\n",
      "Epoch 102/150, Training Loss: 50990.5374, Validation Loss: 268.2890\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\turla\\Documents\\4. Uliege\\Semester 3\\Computer Vision\\Cell-detection-and-counting-Algorithms\\Pipeline\\QuickNN.ipynb Cell 1\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/turla/Documents/4.%20Uliege/Semester%203/Computer%20Vision/Cell-detection-and-counting-Algorithms/Pipeline/QuickNN.ipynb#W0sZmlsZQ%3D%3D?line=128'>129</a>\u001b[0m optimizer \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mAdam(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m0.001\u001b[39m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/turla/Documents/4.%20Uliege/Semester%203/Computer%20Vision/Cell-detection-and-counting-Algorithms/Pipeline/QuickNN.ipynb#W0sZmlsZQ%3D%3D?line=130'>131</a>\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/turla/Documents/4.%20Uliege/Semester%203/Computer%20Vision/Cell-detection-and-counting-Algorithms/Pipeline/QuickNN.ipynb#W0sZmlsZQ%3D%3D?line=131'>132</a>\u001b[0m train_model(model, train_dataloader, test_dataloader, criterion, optimizer, device)\n",
      "\u001b[1;32mc:\\Users\\turla\\Documents\\4. Uliege\\Semester 3\\Computer Vision\\Cell-detection-and-counting-Algorithms\\Pipeline\\QuickNN.ipynb Cell 1\u001b[0m line \u001b[0;36m8\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/turla/Documents/4.%20Uliege/Semester%203/Computer%20Vision/Cell-detection-and-counting-Algorithms/Pipeline/QuickNN.ipynb#W0sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m inputs, labels \u001b[39m=\u001b[39m batch[\u001b[39m'\u001b[39m\u001b[39mimage\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto(device), batch[\u001b[39m'\u001b[39m\u001b[39mcell_count\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/turla/Documents/4.%20Uliege/Semester%203/Computer%20Vision/Cell-detection-and-counting-Algorithms/Pipeline/QuickNN.ipynb#W0sZmlsZQ%3D%3D?line=78'>79</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/turla/Documents/4.%20Uliege/Semester%203/Computer%20Vision/Cell-detection-and-counting-Algorithms/Pipeline/QuickNN.ipynb#W0sZmlsZQ%3D%3D?line=79'>80</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(inputs)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/turla/Documents/4.%20Uliege/Semester%203/Computer%20Vision/Cell-detection-and-counting-Algorithms/Pipeline/QuickNN.ipynb#W0sZmlsZQ%3D%3D?line=80'>81</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/turla/Documents/4.%20Uliege/Semester%203/Computer%20Vision/Cell-detection-and-counting-Algorithms/Pipeline/QuickNN.ipynb#W0sZmlsZQ%3D%3D?line=81'>82</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\turla\\.conda\\envs\\ELEN0016\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\turla\\.conda\\envs\\ELEN0016\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\turla\\Documents\\4. Uliege\\Semester 3\\Computer Vision\\Cell-detection-and-counting-Algorithms\\Pipeline\\QuickNN.ipynb Cell 1\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/turla/Documents/4.%20Uliege/Semester%203/Computer%20Vision/Cell-detection-and-counting-Algorithms/Pipeline/QuickNN.ipynb#W0sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/turla/Documents/4.%20Uliege/Semester%203/Computer%20Vision/Cell-detection-and-counting-Algorithms/Pipeline/QuickNN.ipynb#W0sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m     seg_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49munet(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/turla/Documents/4.%20Uliege/Semester%203/Computer%20Vision/Cell-detection-and-counting-Algorithms/Pipeline/QuickNN.ipynb#W0sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m     mlp_input \u001b[39m=\u001b[39m seg_output\u001b[39m.\u001b[39mview(seg_output\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)  \u001b[39m# Flatten\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/turla/Documents/4.%20Uliege/Semester%203/Computer%20Vision/Cell-detection-and-counting-Algorithms/Pipeline/QuickNN.ipynb#W0sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m     regression_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmlp(mlp_input)\n",
      "File \u001b[1;32mc:\\Users\\turla\\.conda\\envs\\ELEN0016\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\turla\\.conda\\envs\\ELEN0016\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\turla\\.conda\\envs\\ELEN0016\\lib\\site-packages\\segmentation_models_pytorch\\base\\model.py:29\u001b[0m, in \u001b[0;36mSegmentationModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[39m\"\"\"Sequentially pass `x` trough model`s encoder, decoder and heads\"\"\"\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_input_shape(x)\n\u001b[1;32m---> 29\u001b[0m features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(x)\n\u001b[0;32m     30\u001b[0m decoder_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(\u001b[39m*\u001b[39mfeatures)\n\u001b[0;32m     32\u001b[0m masks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msegmentation_head(decoder_output)\n",
      "File \u001b[1;32mc:\\Users\\turla\\.conda\\envs\\ELEN0016\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\turla\\.conda\\envs\\ELEN0016\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\turla\\.conda\\envs\\ELEN0016\\lib\\site-packages\\segmentation_models_pytorch\\encoders\\efficientnet.py:73\u001b[0m, in \u001b[0;36mEfficientNetEncoder.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     71\u001b[0m             drop_connect \u001b[39m=\u001b[39m drop_connect_rate \u001b[39m*\u001b[39m block_number \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_blocks)\n\u001b[0;32m     72\u001b[0m             block_number \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1.0\u001b[39m\n\u001b[1;32m---> 73\u001b[0m             x \u001b[39m=\u001b[39m module(x, drop_connect)\n\u001b[0;32m     75\u001b[0m     features\u001b[39m.\u001b[39mappend(x)\n\u001b[0;32m     77\u001b[0m \u001b[39mreturn\u001b[39;00m features\n",
      "File \u001b[1;32mc:\\Users\\turla\\.conda\\envs\\ELEN0016\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\turla\\.conda\\envs\\ELEN0016\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\turla\\.conda\\envs\\ELEN0016\\lib\\site-packages\\efficientnet_pytorch\\model.py:111\u001b[0m, in \u001b[0;36mMBConvBlock.forward\u001b[1;34m(self, inputs, drop_connect_rate)\u001b[0m\n\u001b[0;32m    109\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_depthwise_conv(x)\n\u001b[0;32m    110\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bn1(x)\n\u001b[1;32m--> 111\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_swish(x)\n\u001b[0;32m    113\u001b[0m \u001b[39m# Squeeze and Excitation\u001b[39;00m\n\u001b[0;32m    114\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhas_se:\n",
      "File \u001b[1;32mc:\\Users\\turla\\.conda\\envs\\ELEN0016\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\turla\\.conda\\envs\\ELEN0016\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\turla\\.conda\\envs\\ELEN0016\\lib\\site-packages\\efficientnet_pytorch\\utils.py:80\u001b[0m, in \u001b[0;36mMemoryEfficientSwish.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m---> 80\u001b[0m     \u001b[39mreturn\u001b[39;00m SwishImplementation\u001b[39m.\u001b[39;49mapply(x)\n",
      "File \u001b[1;32mc:\\Users\\turla\\.conda\\envs\\ELEN0016\\lib\\site-packages\\torch\\autograd\\function.py:539\u001b[0m, in \u001b[0;36mFunction.apply\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_are_functorch_transforms_active():\n\u001b[0;32m    537\u001b[0m     \u001b[39m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[0;32m    538\u001b[0m     args \u001b[39m=\u001b[39m _functorch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[1;32m--> 539\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mapply(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m    541\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39msetup_context \u001b[39m==\u001b[39m _SingleLevelFunction\u001b[39m.\u001b[39msetup_context:\n\u001b[0;32m    542\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    543\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    544\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    545\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mstaticmethod. For more details, please see \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    546\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    547\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\turla\\.conda\\envs\\ELEN0016\\lib\\site-packages\\efficientnet_pytorch\\utils.py:67\u001b[0m, in \u001b[0;36mSwishImplementation.forward\u001b[1;34m(ctx, i)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[0;32m     66\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(ctx, i):\n\u001b[1;32m---> 67\u001b[0m     result \u001b[39m=\u001b[39m i \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39;49msigmoid(i)\n\u001b[0;32m     68\u001b[0m     ctx\u001b[39m.\u001b[39msave_for_backward(i)\n\u001b[0;32m     69\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import segmentation_models_pytorch as smp\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import wandb\n",
    "\n",
    "# Custom dataset class to load data from tensor files\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, folder_path, transform=None):\n",
    "        self.folder_path = folder_path\n",
    "        self.file_list = [f for f in os.listdir(folder_path) if f.endswith('.pt')]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_name = self.file_list[idx]\n",
    "        file_path = os.path.join(self.folder_path, file_name)\n",
    "        data = torch.load(file_path)\n",
    "        image = data[0] \n",
    "        cell_count = torch.tensor(data[1]).float()\n",
    "\n",
    "        # Apply transformation to resize the image to 120x120\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return {'image': image, 'cell_count': cell_count}\n",
    "\n",
    "# Define the U-Net model with a 3-layer MLP for regression\n",
    "class SegmentationAndRegressionModel(nn.Module):\n",
    "    def __init__(self, encoder_name=\"efficientnet-b0\", encoder_weights=\"imagenet\"):\n",
    "        super(SegmentationAndRegressionModel, self).__init__()\n",
    "        self.unet = smp.Unet(encoder_name, encoder_weights=encoder_weights, in_channels=1, classes=1)\n",
    "        for param in self.unet.parameters(): #freeze params for speed\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(16384, 512), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        seg_output = self.unet(x)\n",
    "        mlp_input = seg_output.view(seg_output.size(0), -1)  # Flatten\n",
    "        regression_output = self.mlp(mlp_input)\n",
    "        return regression_output\n",
    "\n",
    "# Function to evaluate the model on the validation set\n",
    "def evaluate_model(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            inputs, labels = batch['image'].to(device), batch['cell_count'].to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "    average_loss = total_loss / len(dataloader)\n",
    "    return average_loss\n",
    "\n",
    "# Function to train the model\n",
    "def train_model(model, train_dataloader, test_dataloader, criterion, optimizer, device, num_epochs=150):\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for batch in train_dataloader:\n",
    "            inputs, labels = batch['image'].to(device), batch['cell_count'].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        epoch_loss = running_loss / len(train_dataloader)\n",
    "\n",
    "        # Log training loss to WandB\n",
    "        wandb.log({\"Training Loss\": epoch_loss})\n",
    "\n",
    "        # Evaluate the model on the validation set\n",
    "        val_loss = evaluate_model(model, test_dataloader, criterion, device)\n",
    "\n",
    "        # Log validation loss to WandB\n",
    "        wandb.log({\"Validation Loss\": val_loss})\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Training Loss: {epoch_loss:.4f}, Validation Loss: {val_loss:.4f}')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set your folder path containing the PyTorch tensor files\n",
    "    train_data_folder = \"../Data/input_tensors/train\"\n",
    "    test_data_folder = \"../Data/input_tensors/val\"\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),  \n",
    "        transforms.Resize((128, 128)),  # Resize the image for consistent input shape\n",
    "        transforms.ToTensor(), \n",
    "    ])\n",
    "\n",
    "    # Create custom datasets for train and test\n",
    "    train_dataset = CustomDataset(folder_path=train_data_folder, transform=transform)\n",
    "    test_dataset = CustomDataset(folder_path=test_data_folder, transform=transform)\n",
    "\n",
    "    # Create DataLoader for train and test datasets\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "    # Initialize WandB\n",
    "    wandb.init(project=\"Count_cells\", entity=\"turlagheoin\")  # allow to show on wandb\n",
    "\n",
    "    # Initialize the model\n",
    "    model = SegmentationAndRegressionModel()\n",
    "\n",
    "    # Check if GPU is available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Train the model\n",
    "    train_model(model, train_dataloader, test_dataloader, criterion, optimizer, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model for later use once training is done\n",
    "\n",
    "torch.save(model.state_dict(), 'countingModel.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ELEN0016",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
